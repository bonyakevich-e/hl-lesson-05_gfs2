---
- hosts: all
  remote_user: ${remote_user}
  become: yes
  gather_facts: no
  tasks:

  - name: Wait for system to become reachable
    ansible.builtin.wait_for_connection:

  - name: Gather facts manually
    ansible.builtin.setup:
  
  - name: Set timezone
    timezone:
      name: Europe/Moscow
  
  - name: Add entries to hosts
    copy:
      dest: /etc/hosts
      content: "#\n
127.0.0.1	localhost\n
%{ for node in nodes ~}
${node.network_interface.0.ip_address}	${node.hostname}\n
%{ endfor ~}
${iscsi.network_interface.0.ip_address}	${iscsi.hostname}\n
"

- hosts: iscsi
  remote_user: ${remote_user}
  become: yes
  tasks:
 
  - name: Install targetcli-fb
    apt:
      name: targetcli-fb
      state: present
# настраиваем iscsi target с помощью сгенерированного скрипта   
  - name: Configure target using bash script
    script: iscsi_target.bash

- hosts: cluster
  remote_user: ${remote_user}
  become: yes
  vars:
  - iqn_base: ${iqn_base}
  tasks:
  
  - name: Install packages
    apt:
      name: "{{ item }}"
      state: latest
    with_items:
    - pacemaker
    - pcs
    - gfs2-utils
    - open-iscsi
    - lvm2
    - dlm-controld
    - lvm2-lockd
    - resource-agents-extra
    - resource-agents-common
    - resource-agents-base
    - watchdog 
    - pcp-zeroconf
    - fence-agents-scsi

# меняем iqn iscsi клиентов
  - name: Change InitiatorName in initiatorname.iscsi
    ansible.builtin.lineinfile:
      path: /etc/iscsi/initiatorname.iscsi
      regexp: '^InitiatorName='
      line: InitiatorName=${iqn_base}:${cluster_name}.{{ inventory_hostname }}

  - name: restart iscsid
    service: 
      name: iscsid
      state: restarted

# подключаем iscsi диск на ноды
  - name: Discover iscsi target
    community.general.open_iscsi:
      show_nodes: true
      discover: true
      portal: ${iscsi.hostname}

  - name: Connect to the target  
    community.general.open_iscsi:
      login: true
      target: '${iqn_base}:storage.target00'

# автозапуск и автоподключение iscsi диска при загрузке системы
  - name: Start iscsi
    service:
      name: "{{ item }}"
      state: started
      enabled: yes
    with_items:
      - iscsi

# по дефолту Pacemaker запускает свою службу под пользователем hacluster
  - name: Set password for hacluster to '123'
    user:
      name: hacluster
      password: $5$A55.Uz8o.y8MuGaf$w3axEzoOgSeGyJo3OE56a4Ki1ctGEWP1GMyU7tOVJu6

#  - name: Start cluster services
#    service:
#      name: "{{ item }}"
#      state: started
#      enabled: yes
#    with_items:
#      - pcsd

# настраиваем Corosync. Аутентифицируем ноды, которые будут составлять наш кластер
#  - name: Set up corosync nodes
#    run_once: true
#    command:
#      cmd: /sbin/pcs host auth -u hacluster -p 123 %{for node in nodes}${node.hostname} %{endfor}
#      creates: /var/lib/pcsd/tokens

# Настраиваем файл конфигурации кластера и синхронизируем конфигурацию с указанными узлами
#  - name: Create cluster
#    run_once: true
#    command:
#      cmd: pcs cluster setup ${cluster_name} --start --enable%{for node in nodes} ${node.hostname}%{endfor} --force
#      creates: /etc/corosync/corosync.conf

# Отключаем STONIGHT, он же FENCING
#  - name: Disable fencing
#    run_once: true
#    command: /sbin/pcs property set stonith-enabled=false

# Если кворум не соблюдается, замораживаем кластер
#  - name: Freeze a no-quorum policy
#    run_once: true
#    command: /sbin/pcs property set no-quorum-policy=freeze

# Создаем ресурс dlm
#  - name: Create dlm
#    run_once: true
#    command: /sbin/pcs resource create dlm ocf:pacemaker:controld op start timeout=90s interval=0 op stop timeout=100s interval=0
#    register: result
#    failed_when:
#      - result.rc != 0 and "already exists" not in result.stderr

# Создаем ресурс lvmlockd
#  - name: Create clvmd
#    run_once: true
#    command: /sbin/pcs resource create lvmlockd ocf:heartbeat:lvmlockd op start timeout=90s interval=0 op stop timeout=100s interval=0
#    register: result
#    failed_when:
#      - result.rc != 0 and "already exists" not in result.stderr

# указываем порядок запуска ресурсов на нодах
#  - name: Set order constraint
#    run_once: true
#    command: pcs constraint order start dlm then lvmlockd
#    register: result
#    failed_when:
#    - result.rc != 0 and "already exists" not in result.stderr

# указываем какие ресурсы размещать на одной ноде
#  - name: Set colocation constraint
#    run_once: true
#    command: pcs constraint colocation add lvmlockd with dlm
#    register: result
#    failed_when:
#    - result.rc != 0 and "already exists" not in result.stderr

# создаем кластерный VG на iscsi диске
#  - name: Create a PV and VG
#    run_once: true
#    community.general.lvg:
#      pvs: /dev/sda
#      vg: ${vg_name}
#      vg_options: --shared

# создаем LV на только что созданном кластерном VG
#  - name: Create a LV
#    run_once: true
#    community.general.lvol:
#      vg: ${vg_name}
#      lv: ${lv_name}
#      size: 100%VG

# создаем кластерную файловую систему gfs2 на только что созданном LV
#  - name: Create a FS
#    run_once: true
#    command: mkfs.gfs2 -j ${cluster_size} -p lock_dlm -t ${cluster_name}:${fs_name} -O /dev/${vg_name}/${lv_name}
#    register: result
#    failed_when:
#    - result.rc != 0
#    - '"Device or resource busy" not in result.stderr'

# Создаем ресурс clusterfs
#  - name: Create a clusterfs resource
#    run_once: true
#    command: /sbin/pcs resource create clusterfs ocf:heartbeat:Filesystem device=/dev/${vg_name}/${lv_name} directory=/mnt/gfs2 fstype=gfs2 options=noatime op monitor interval=10s on-fail=ignore clone interleave=true --wait
#    register: result
#    failed_when:
#    - result.rc != 0
#    - '"already exists" not in result.stderr'

# указываем порядок запуска ресурсов на нодах
#  - name: Set order constraint
#    run_once: true
#    command: pcs constraint order start clvmd-clone then clusterfs-clone
#    register: result
#    failed_when:
#    - result.rc != 0
#    - '"already exists" not in result.stderr'

# указываем какие ресурсы размещать на одной ноде
#  - name: Set colocation constraint
#    run_once: true
#    command: pcs constraint colocation add clusterfs-clone with clvmd-clone
#    register: result
#    failed_when:
#    - result.rc != 0
#    - '"already exists" not in result.stderr'

# создаем файлы с именем ноды на расшаренном диске для демонстрации одновременного доступа
#  - name: Put a test file
#    shell:
#      cmd: hostname >> hostnames
#      chdir: /mnt/gfs2