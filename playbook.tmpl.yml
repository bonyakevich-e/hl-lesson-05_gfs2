---
- hosts: all
  remote_user: ${remote_user}
  become: yes
  gather_facts: no
  tasks:

  - name: Wait for system to become reachable
    ansible.builtin.wait_for_connection:

  - name: Gather facts manually
    ansible.builtin.setup:
  
  - name: Set timezone
    timezone:
      name: Europe/Moscow
  
  - name: Add entries to hosts
    copy:
      dest: /etc/hosts
      content: "#\n
127.0.0.1	localhost\n
%{ for node in nodes ~}
${node.network_interface.0.ip_address}	${node.hostname}\n
%{ endfor ~}
${iscsi.network_interface.0.ip_address}	${iscsi.hostname}\n
"

- hosts: iscsi
  remote_user: ${remote_user}
  become: yes
  tasks:
 
  - name: Install targetcli-fb
    apt:
      name: targetcli-fb
      state: present
# настраиваем iscsi target с помощью сгенерированного скрипта   
  - name: Configure target using bash script
    script: iscsi_target.bash

- hosts: cluster
  remote_user: ${remote_user}
  become: yes
  vars:
  - iqn_base: ${iqn_base}
  tasks:
  
  - name: Install packages
    apt:
      name: "{{ item }}"
      state: latest
    with_items:
    - pacemaker
    - pcs
    - gfs2-utils
    - open-iscsi
    - lvm2

# меняем iqn iscsi клиентов
  - name: Change InitiatorName in initiatorname.iscsi
    ansible.builtin.lineinfile:
      path: /etc/iscsi/initiatorname.iscsi
      regexp: '^InitiatorName='
      line: InitiatorName=${iqn_base}:${cluster_name}.{{ inventory_hostname }}

  - name: restart iscsid
    service: 
      name: iscsid
      state: restarted

# подключаем iscsi диск на ноды
  - name: Discover iscsi target
    community.general.open_iscsi:
      show_nodes: true
      discover: true
      portal: ${iscsi.hostname}

  - name: Connect to the target  
    community.general.open_iscsi:
      login: true
      target: '${iqn_base}:storage.target00'

# автозапуск и автоподключение iscsi диска при загрузке системы
  - name: Start iscsi
    service:
      name: "{{ item }}"
      state: started
      enabled: yes
    with_items:
      - iscsi

# по дефолту Pacemaker запускает свою службу под пользователем hacluster
  - name: Set password for hacluster to '123'
    user:
      name: hacluster
      password: $5$A55.Uz8o.y8MuGaf$w3axEzoOgSeGyJo3OE56a4Ki1ctGEWP1GMyU7tOVJu6

  - name: Start cluster services
    service:
      name: "{{ item }}"
      state: started
      enabled: yes
    with_items:
      - pcsd

# настраиваем Corosync. Аутентифицируем ноды, которые будут составлять наш кластер
  - name: Set up corosync nodes
    run_once: true
    command:
      cmd: /sbin/pcs host auth -u hacluster -p 123 %{for node in nodes}${node.hostname} %{endfor}
#      creates: /var/lib/pcsd/tokens

# Настраиваем файл конфигурации кластера и синхронизируем конфигурацию с указанными узлами
  - name: Create cluster
    run_once: true
    command:
      cmd: /sbin/pcs cluster setup --name ${cluster_name} --start --wait --enable%{for node in nodes} ${node.hostname},${node.network_interface.0.ip_address}%{endfor}
      creates: /etc/corosync/corosync.conf

# Отключаем STONIGHT, он же FENCING
  - name: Disable fencing
    run_once: true
    command: /sbin/pcs property set stonith-enabled=false

# Если кворум не соблюдается, замораживаем кластер
  - name: Freeze a no-quorum policy
    run_once: true
    command: /sbin/pcs property set no-quorum-policy=freeze

# Создаем ресурс dlm
  - name: Create dlm
    run_once: true
    command: /sbin/pcs resource create dlm systemd:dlm op monitor interval=30s on-fail=ignore clone interleave=true ordered=true --wait
    register: result
    failed_when:
      - result.rc != 0 and "already exists" not in result.stderr

# Создаем ресурс clvmd
  - name: Create clvmd
    run_once: true
    command: /sbin/pcs resource create clvmd ocf:heartbeat:clvm op monitor interval=30s on-fail=ignore clone interleave=true ordered=true --wait
    register: result
    failed_when:
      - result.rc != 0 and "already exists" not in result.stderr

# указываем порядок запуска ресурсов на нодах
  - name: Set order constraint
    run_once: true
    command: pcs constraint order start dlm-clone then clvmd-clone
    register: result
    failed_when:
    - result.rc != 0 and "already exists" not in result.stderr

# указываем какие ресурсы размещать на одной ноде
  - name: Set colocation constraint
    run_once: true
    command: pcs constraint colocation add clvmd-clone with dlm-clone
    register: result
    failed_when:
    - result.rc != 0 and "already exists" not in result.stderr

# создаем кластерный VG на iscsi диске
  - name: Create a PV and VG
    run_once: true
    community.general.lvg:
      pvs: /dev/sda
      vg: ${vg_name}
      vg_options: --clustered=y --autobackup=y

# создаем LV на только что созданном кластерном VG
  - name: Create a LV
    run_once: true
    community.general.lvol:
      vg: ${vg_name}
      lv: ${lv_name}
      size: 100%VG

# создаем кластерную файловую систему gfs2 на только что созданном LV
  - name: Create a FS
    run_once: true
    command: mkfs.gfs2 -j ${cluster_size} -p lock_dlm -t ${cluster_name}:${fs_name} -O /dev/${vg_name}/${lv_name}
    register: result
    failed_when:
    - result.rc != 0
    - '"Device or resource busy" not in result.stderr'

# Создаем ресурс clusterfs
  - name: Create a clusterfs resource
    run_once: true
    command: /sbin/pcs resource create clusterfs ocf:heartbeat:Filesystem device=/dev/${vg_name}/${lv_name} directory=/mnt/gfs2 fstype=gfs2 options=noatime op monitor interval=10s on-fail=ignore clone interleave=true --wait
    register: result
    failed_when:
    - result.rc != 0
    - '"already exists" not in result.stderr'

# указываем порядок запуска ресурсов на нодах
  - name: Set order constraint
    run_once: true
    command: pcs constraint order start clvmd-clone then clusterfs-clone
    register: result
    failed_when:
    - result.rc != 0
    - '"already exists" not in result.stderr'

# указываем какие ресурсы размещать на одной ноде
  - name: Set colocation constraint
    run_once: true
    command: pcs constraint colocation add clusterfs-clone with clvmd-clone
    register: result
    failed_when:
    - result.rc != 0
    - '"already exists" not in result.stderr'

# создаем файлы с именем ноды на расшаренном диске для демонстрации одновременного доступа
  - name: Put a test file
    shell:
      cmd: hostname >> hostnames
      chdir: /mnt/gfs2