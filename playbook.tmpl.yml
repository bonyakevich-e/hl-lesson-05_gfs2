---
- hosts: all
  remote_user: ${remote_user}
  become: yes
  gather_facts: no
  tasks:
  
# ждем пока виртуальные машины придут в себя. иначе могу быть баги с утановкой пакетов  
  - name: Pause for 2 minutes
    ansible.builtin.pause:
      minutes: 2

  - name: Wait for system to become reachable
    ansible.builtin.wait_for_connection:

  - name: Gather facts manually
    ansible.builtin.setup:
  
  - name: Set timezone
    timezone:
      name: Europe/Moscow
  
  - name: Add entries to hosts
    copy:
      dest: /etc/hosts
      content: "#\n
127.0.0.1	localhost\n
%{ for node in nodes ~}
${node.network_interface.0.ip_address}	${node.hostname}\n
%{ endfor ~}
${iscsi.network_interface.0.ip_address}	${iscsi.hostname}\n
"

- hosts: iscsi
  remote_user: ${remote_user}
  become: yes
  tasks:
 
  - name: Install targetcli-fb
    apt:
      name: targetcli-fb
      state: present
# настраиваем iscsi target с помощью сгенерированного скрипта   
  - name: Configure target using bash script
    script: iscsi_target.bash

- hosts: cluster
  remote_user: ${remote_user}
  become: yes
  vars:
  - iqn_base: ${iqn_base}
  tasks:
  
  - name: Install packages
    apt:
      name: "{{ item }}"
      state: latest
    with_items:
    - pacemaker
    - pcs
    - gfs2-utils
    - open-iscsi
    - lvm2
    - dlm-controld
    - lvm2-lockd
    - resource-agents-extra
    - resource-agents-common
    - resource-agents-base
    - watchdog 
    - pcp-zeroconf
    - fence-agents-scsi

# меняем iqn iscsi клиентов
  - name: Change InitiatorName in initiatorname.iscsi
    ansible.builtin.lineinfile:
      path: /etc/iscsi/initiatorname.iscsi
      regexp: '^InitiatorName='
      line: InitiatorName=${iqn_base}:${cluster_name}.{{ inventory_hostname }}

  - name: restart iscsid
    service: 
      name: iscsid
      state: restarted

# подключаем iscsi диск на ноды
  - name: Discover iscsi target
    community.general.open_iscsi:
      show_nodes: true
      discover: true
      portal: ${iscsi.hostname}

  - name: Connect to the target  
    community.general.open_iscsi:
      login: true
      target: '${iqn_base}:storage.target00'

# автозапуск и автоподключение iscsi диска при загрузке системы
  - name: Start iscsi
    service:
      name: "{{ item }}"
      state: started
      enabled: yes
    with_items:
      - iscsi

# по дефолту Pacemaker запускает свою службу под пользователем hacluster
  - name: Set password for hacluster to '123'
    user:
      name: hacluster
      password: $5$A55.Uz8o.y8MuGaf$w3axEzoOgSeGyJo3OE56a4Ki1ctGEWP1GMyU7tOVJu6

# стартуем pcsd
  - name: Start cluster services
    service:
      name: "{{ item }}"
      state: started
      enabled: yes
    with_items:
      - pcsd

# аутентифицируем ноды, которые будут составлять наш кластер
  - name: authorize among nodes
    run_once: true
    command:
      cmd: /sbin/pcs host auth -u hacluster -p 123 %{for node in nodes}${node.hostname} %{endfor}
      creates: /var/lib/pcsd/tokens

# настраиваем кластер
  - name: configure cluster
    tags: set cluster
    run_once: true
    command:
      cmd: pcs cluster setup ${cluster_name} --start --enable%{for node in nodes} ${node.hostname}%{endfor} --force
      

# копируем fencing скрипт для мониторинга iscsi в watchdog.d
  - name: copy fence_scsi_check script to watchdog.d directory
    tags: copy fence_scsi_check
    ansible.builtin.copy:
      src: /usr/share/cluster/fence_scsi_check
      dest: /etc/watchdog.d/
      remote_src: yes

# запускаем watchdog
  - name: start watchdog
    tags: start watchdog
    ansible.builtin.service:
      name: watchdog
      state: started
      enabled: yes

# получаем wwn-идентификатор iscsi диска
  - name: confirm iscsi disk ID
    tags: wwn-id
    run_once: true
    ansible.builtin.shell: ls -l /dev/disk/by-id | grep sda | grep wwn | cut -d ' ' -f 10
    register: wwn_id

  - name: show wwn id
    tags: wwn-id
    run_once: true
    ansible.builtin.debug: 
      msg: WWN-ID  {{wwn_id.stdout}}
    

# настраиваем fencing
  - name: set fencing
    tags: set fencing
    run_once: true
    ansible.builtin.command: 
      cmd: pcs stonith create scsi-shooter fence_scsi pcmk_host_list="%{for node in nodes} ${node.hostname}%{endfor}" devices=/dev/disk/by-id/{{wwn_id.stdout}} meta provides=unfencing
    
# включаем поддержку lvmlockd
  - name: uncomment use_lvmlockd in lvm.conf
    tags: set use_lvmlockd
    ansible.builtin.lineinfile:
      dest: /etc/lvm/lvm.conf
      regexp: '#\s*use_lvmlockd = 0'
      line: 'use_lvmlockd = 1'

# устанавливаем [no-quorum-policy=freeze] на GFS2
  - name: Freeze a no-quorum policy
    tags: set no-quorum-policy
    run_once: true
    command: /sbin/pcs property set no-quorum-policy=freeze

# Создаем ресурс controld
  - name: create controld resource
    tags: set controld
    run_once: true
    command: /sbin/pcs resource create dlm ocf:pacemaker:controld op monitor interval=30s on-fail=fence group locking --future
    register: result
    failed_when:
      - result.rc != 0 and "already exists" not in result.stderr

  - name: create clone of [locking] to activate it on all nodes in cluster
    tags: set locking
    run_once: true
    command: pcs resource clone locking interleave=true

# Создаем ресурс lvmlockd
  - name: create lvmlockd resource
    tags: set lvmlockd
    run_once: true
    command: /sbin/pcs resource create lvmlockdd ocf:heartbeat:lvmlockd op monitor interval=30s on-fail=fence group locking --future
    register: result
    failed_when:
      - result.rc != 0 and "already exists" not in result.stderr

# создаем кластерный VG на iscsi диске
  - name: create a PV and VG
    tags: set vg
    run_once: true
    community.general.lvg:
      pvs: /dev/sda
      vg: ${vg_name}
      vg_options: --shared

# на всех нодах запускаем lock manager для iscsi диска
  - name: start lock manager for shared volume
    tags: set lockmanager
    command: vgchange --lock-start ${vg_name} 

# создаем LV 
  - name: create logical volume
    tags: set lv
    run_once: true
    community.general.lvol:
      vg: ${vg_name}
      lv: ${lv_name}
      size: 100%VG

# создаем кластерную файловую систему gfs2 на только что созданном LV
  - name: create a FS
    tags: set fs
    run_once: true
    command: mkfs.gfs2 -j ${cluster_size} -p lock_dlm -t ${cluster_name}:${fs_name} -O /dev/${vg_name}/${lv_name}
    register: result
    failed_when:
    - result.rc != 0
    - '"Device or resource busy" not in result.stderr'

# создаем LVM-activate ресурс
  - name: create LVM-activate resource
    tags: set LVM-activate
    run_once: true
    command: pcs resource create shared_lv ocf:heartbeat:LVM-activate lvname=${lv_name} vgname=${vg_name} activation_mode=shared vg_access_mode=lvmlockd group shared_vg --future

  - name: create clone of [LVM-activate]
    tags: set LVM-activate
    run_once: true
    command: pcs resource clone shared_vg interleave=true

# устанавливаем порядок запуска ресурсов
  - name: set that [shared_vg] and [locking] start on a same node
    tags: set constraint
    run_once: true
    command: pcs constraint colocation add shared_vg-clone with locking-clone
    register: result
    failed_when:
      - result.rc != 0 and "already exists" not in result.stderr

# создаем Filesystem ресурс
  - name: create Filesystem resource
    tags: create filesystem resource
    run_once: true
    command: pcs resource create shared_fs ocf:heartbeat:Filesystem device="/dev/${vg_name}/${lv_name}" directory="/mnt" fstype="gfs2" options=noatime op monitor interval=10s on-fail=fence group shared_vg --future

  - name: Pause for 10 seconds
    ansible.builtin.pause:
      seconds: 10

# создаем файлы с именем ноды на расшаренном диске для демонстрации одновременного доступа
  - name: put a test file
    tags: put a test file
    shell:
      cmd: hostname >> hostnames
      chdir: /mnt